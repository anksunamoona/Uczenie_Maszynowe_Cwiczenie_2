{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_IHsmTIfmPd",
        "outputId": "a28d61d9-427a-4f49-9724-43c7525779bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.7 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.13.0rc0, 2.13.0rc1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.7\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.7 keras gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1T5Cm4Tt1sO21ARz6BvStSW4yks118rRz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myIUHePufzeQ",
        "outputId": "98879e7d-9338-4700-9636-bb0fb32d1318"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1T5Cm4Tt1sO21ARz6BvStSW4yks118rRz\n",
            "To: /content/small_data_set.zip\n",
            "100% 2.07G/2.07G [00:12<00:00, 161MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "class ImageClassifier:\n",
        "    def __init__(self, zip_file_path, dataset_folder=\"dataset\", image_size=(150, 150), batch_size=32):\n",
        "        self.zip_file_path = zip_file_path\n",
        "        self.dataset_folder = dataset_folder\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def extract_zip(self):\n",
        "        with zipfile.ZipFile(self.zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.dataset_folder)\n",
        "        print(\"Dataset extracted successfully.\")\n",
        "\n",
        "    def prepare_data(self):\n",
        "        data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "        self.train_data = data_gen.flow_from_directory(\n",
        "            self.dataset_folder,\n",
        "            target_size=self.image_size,\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode=\"categorical\",\n",
        "            subset=\"training\"\n",
        "        )\n",
        "\n",
        "        self.validation_data = data_gen.flow_from_directory(\n",
        "            self.dataset_folder,\n",
        "            target_size=self.image_size,\n",
        "            batch_size=self.batch_size,\n",
        "            class_mode=\"categorical\",\n",
        "            subset=\"validation\"\n",
        "        )\n",
        "        self.num_classes = len(self.train_data.class_indices)\n",
        "        print(f\"Data prepared for training and validation. Number of classes: {self.num_classes}\")\n",
        "\n",
        "    def create_model(self):\n",
        "        self.model = models.Sequential([\n",
        "            layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(*self.image_size, 3)),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(512, activation=\"relu\"),\n",
        "            layers.Dense(self.num_classes, activation=\"softmax\")\n",
        "        ])\n",
        "\n",
        "        self.model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        print(\"Deep neural network model created and compiled.\")\n",
        "\n",
        "    def train(self, epochs=10):\n",
        "        self.model.fit(\n",
        "            self.train_data,\n",
        "            epochs=epochs,\n",
        "            validation_data=self.validation_data\n",
        "        )\n",
        "        print(\"Model trained successfully.\")\n",
        "\n",
        "# Usage\n",
        "zip_file_path = \"/content/small_data_set.zip\"\n",
        "\n",
        "classifier = ImageClassifier(zip_file_path)\n",
        "classifier.extract_zip()\n",
        "classifier.prepare_data()\n",
        "classifier.create_model()\n",
        "classifier.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbGSHVXefzi6",
        "outputId": "0d59708a-2bd7-4789-ca38-04be462e7ceb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully.\n",
            "Found 200 images belonging to 5 classes.\n",
            "Found 50 images belonging to 5 classes.\n",
            "Data prepared for training and validation. Number of classes: 5\n",
            "Deep neural network model created and compiled.\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 151s 22s/step - loss: 2.0075 - accuracy: 0.1500 - val_loss: 1.5761 - val_accuracy: 0.2200\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 148s 22s/step - loss: 1.5502 - accuracy: 0.2350 - val_loss: 1.5810 - val_accuracy: 0.2600\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 147s 22s/step - loss: 1.5164 - accuracy: 0.3400 - val_loss: 1.4781 - val_accuracy: 0.2800\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 148s 24s/step - loss: 1.3762 - accuracy: 0.4100 - val_loss: 1.3821 - val_accuracy: 0.3600\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 147s 22s/step - loss: 1.2055 - accuracy: 0.5250 - val_loss: 1.4354 - val_accuracy: 0.3600\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 151s 22s/step - loss: 1.0585 - accuracy: 0.6200 - val_loss: 1.4680 - val_accuracy: 0.4800\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 148s 24s/step - loss: 0.9043 - accuracy: 0.6600 - val_loss: 1.4405 - val_accuracy: 0.4200\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 151s 22s/step - loss: 0.6437 - accuracy: 0.7700 - val_loss: 1.7499 - val_accuracy: 0.3600\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 148s 24s/step - loss: 0.5610 - accuracy: 0.7950 - val_loss: 1.7225 - val_accuracy: 0.4600\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 149s 22s/step - loss: 0.3295 - accuracy: 0.9250 - val_loss: 1.7270 - val_accuracy: 0.5000\n",
            "Model trained successfully.\n"
          ]
        }
      ]
    }
  ]
}